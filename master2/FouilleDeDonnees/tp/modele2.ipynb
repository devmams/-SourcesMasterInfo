{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:2.1m;\"> 1- Modèle de prediction 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:1.5m;\"> 1- Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mamdiallo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier = open(\"CorpusM2-AFD/corpus.tache1.learn\",\"r\",encoding = \"latin-1\")\n",
    "lignes = fichier.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for ligne in lignes:\n",
    "    ligne = ligne.replace('\\n','')\n",
    "    data.append(ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<100:1:C> Quand je dis chers amis, il ne s'agit pas là d'une formule diplomatique, mais de l'expression de ce que je ressens.\n",
      "dis\n",
      "dis\n",
      "diplomat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "print(data[0])\n",
    "print(data[0].split(' ')[3])\n",
    "print(stemmer.stem(data[0].split(' ')[3]))\n",
    "print(stemmer.stem('diplomatique'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroOuUn(c):\n",
    "    if c == 'C':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupPersonne(s):\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i]=='>':\n",
    "            return(zeroOuUn(s[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupDonnees():\n",
    "    X = []\n",
    "    y = []\n",
    "    for doc in data:\n",
    "        doc1 = doc.split(' ')\n",
    "    #     doc1 =  [stemmer.stem(m) for m in doc1] #lemmetisation\n",
    "    #     print(\" \".join(doc1[1:]))\n",
    "        res = \" \".join(doc1[1:])        \n",
    "        res = res.lower() #convertit les Majuscules en miniscules\n",
    "        res = unidecode(res) #suppression des accents et cedilles\n",
    "        res = res.translate(str.maketrans('', '', string.punctuation)) #suprression des ponctuations\n",
    "        res = res.split(' ')\n",
    "        res = \" \".join([mot for mot in res if mot not in stopwords]) #suppression des stopwords\n",
    "        X.append(res)\n",
    "        y.append(recupPersonne(doc1[0]))\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = recupDonnees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57413\n",
      "57413\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  -  quand dis chers amis sagit dune formule diplomatique lexpression ressens\n",
      "1  -  dabord merci cet exceptionnel accueil congolais brazavillois reserve cet apresmidi\n",
      "0  -  cest vrai quil a tres souvent pays refus regarder droit devant soi comme souhait den rester certaine peur changement cest vrai\n"
     ]
    }
   ],
   "source": [
    "print(y[0], \" - \",X[0])\n",
    "print(y[1], \" - \",X[1])\n",
    "print(y[12], \" - \",X[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45930\n",
      "45930\n",
      "11483\n",
      "11483\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "\n",
    "print(type(X_test))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_max = 2500\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = TfidfVectorizer(min_df=5, max_df=0.7)\n",
    "vectorizer = TfidfVectorizer(max_features=mot_max, min_df=5, max_df=0.7)\n",
    "X_train_tfifd = vectorizer.fit_transform(X_train)\n",
    "X_test_tfifd = vectorizer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_tfifd.toarray()))\n",
    "print(type(X_test_tfifd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merite attention appui\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(X_train_tfifd.toarray()[970])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45930, 2500)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfifd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as utils\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "# print(torch.get_default_dtype())\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_data_tensor = torch.tensor(X_train_tfifd.toarray())\n",
    "train_target_tensor = torch.tensor(np.asarray(y_train)) # transform to torch tensors\n",
    "\n",
    "test_data_tensor = torch.tensor(X_test_tfifd.toarray()) # transform to torch tensors\n",
    "test_target_tensor = torch.tensor(np.asarray(y_test)) # transform to torch tensors\n",
    "\n",
    "train_dataset = utils.TensorDataset(train_data_tensor,train_target_tensor) # create your datset\n",
    "test_dataset = utils.TensorDataset(test_data_tensor,test_target_tensor) # create your datset\n",
    "\n",
    "batch_size = 64\n",
    "# Set the training loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# Set the testing loader\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = torch.randn(mot_max, 15, requires_grad=True)\n",
    "weights2 = torch.randn(15, 15, requires_grad=True)\n",
    "weights3 = torch.randn(15, 2, requires_grad=True)\n",
    "\n",
    "b1 = torch.randn((1, 15), requires_grad=True) # bias for hidden layer\n",
    "b2 = torch.randn((1, 15), requires_grad=True) # bias for output layer\n",
    "b3 = torch.randn((1, 2), requires_grad=True) # bias for output layer\n",
    "\n",
    "# b1 = torch.randn(500,)\n",
    "# b2 = torch.randn((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    return 1 / (1 + torch.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy on test set 0.6771749542802403\n"
     ]
    }
   ],
   "source": [
    "def test(weights1, weights2,weights3,b1, b2, b3, test_loader):\n",
    "    test_size = len(test_loader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        #print(batch_idx, data.shape, target.shape)\n",
    "#         data = data.view((-1, 28*28))\n",
    "        #print(batch_idx, data.shape, target.shape)\n",
    "        data = data.float()\n",
    "        target = target.to(dtype=torch.long)        \n",
    "        z1 = torch.matmul(data, weights1) + b1\n",
    "        a1 = sigmoid_activation(z1)\n",
    "        \n",
    "        z2 = torch.matmul(a1, weights2) + b2\n",
    "        a2 = sigmoid_activation(z2)\n",
    "\n",
    "        z3 = torch.matmul(a2, weights3) + b3\n",
    "        \n",
    "        outputs = sigmoid_activation(z3)\n",
    "        \n",
    "        softmax = F.softmax(outputs, dim=1)\n",
    "        pred = softmax.argmax(dim=1, keepdim=True)\n",
    "        n_correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        correct += n_correct\n",
    "\n",
    "    acc = correct / test_size\n",
    "    print(\" Accuracy on test set\", acc)\n",
    "    return\n",
    "\n",
    "test(weights1, weights2,weights3,b1, b2, b3, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss shape: 0.6479528546333313 Accuracy on test set 0.8400243838718106\n",
      "Loss shape: 0.5965312123298645 Accuracy on test set 0.8666724723504311\n",
      "Loss shape: 0.5599048137664795 Accuracy on test set 0.8685012627362187\n",
      "Loss shape: 0.5551152229309082 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.5301421284675598 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.50790590047836343 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.55523848533630375 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.46998408436775213 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.47692486643791265 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.46575838327407837 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.43899309635162354 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.43734255433082587 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.48982185125350955 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.51422810554504495 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.54511672258377085 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41619357466697693 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.50940269231796265 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.43896460533142097 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41220384836196945 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.42492470145225525 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.43818095326423645 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.42067229747772217 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.37900471687316895 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.40549403429031375 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.46487972140312195 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.50836616754531863 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.52095943689346316 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44787827134132385 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44660204648971563 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44665083289146423 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.43185272812843325 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.43256822228431756 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.52082067728042654 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.50701153278350834 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41731786727905273 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.50529104471206677 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.51891845464706425 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.51941663026809694 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.50539815425872865 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.43124750256538394 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.47535118460655215 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.38533112406730653 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44435581564903267 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41359329223632816 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.39970189332962036 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41476660966873176 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.47435677051544194 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.40029466152191164 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44340416789054875 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.53333067893981935 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.42970687150955204 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.47293689846992497 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41371488571166994 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41321402788162236 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.53489470481872564 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.51903611421585084 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41289362311363227 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44350585341453555 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41255336999893195 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44367045164108276 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.38283184170722963 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44274413585662844 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.45771872997283936 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.45765802264213563 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.41274172067642214 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.42701503634452826 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.39685583114624023 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.47315967082977295 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.44254559278488167 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.42762908339500434 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.42736575007438667 Accuracy on test set 0.8685883479926848\n",
      "Loss shape: 0.45937642455101013"
     ]
    }
   ],
   "source": [
    "it = 0\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    # Be sure to start the loop with zeros grad\n",
    "    if weights1.grad is not None:\n",
    "        weights1.grad.zero_()\n",
    "    \n",
    "    if weights2.grad is not None:\n",
    "        weights2.grad.zero_()\n",
    "        \n",
    "    if weights3.grad is not None:\n",
    "        weights3.grad.zero_()\n",
    "    \n",
    "    if b1.grad is not None:\n",
    "        b1.grad.zero_()\n",
    "        \n",
    "    if b2.grad is not None:\n",
    "        b2.grad.zero_()\n",
    "        \n",
    "    if b3.grad is not None:\n",
    "        b3.grad.zero_()\n",
    "        \n",
    "        \n",
    "    #print(\"batch_idx: {}, data.shape: {}, target.shape: {}\".format(batch_idx, data.shape, targets.shape))\n",
    "    data = data.float()\n",
    "    targets = targets.to(dtype=torch.long)\n",
    "    z1 = torch.matmul(data, weights1) + b1\n",
    "    a1 = sigmoid_activation(z1)\n",
    "        \n",
    "    z2 = torch.matmul(a1, weights2) + b2\n",
    "    a2 = sigmoid_activation(z2)\n",
    "\n",
    "    z3 = torch.matmul(a2, weights3) + b3\n",
    "        \n",
    "    outputs = sigmoid_activation(z3)\n",
    "    log_softmax = F.log_softmax(outputs, dim=1)\n",
    "    #print(\"Log softmax: {}\".format(log_softmax.shape))\n",
    "\n",
    "    #print((-log_softmax[0][targets[0]] + -log_softmax[1][targets[1]] )  / 2 )\n",
    "    #print(-log_softmax[0][targets[0]], targets[0])\n",
    "    \n",
    "    loss = F.nll_loss(log_softmax, targets)\n",
    "    print(\"\\rLoss shape: {}\".format(loss), end=\"\")\n",
    "    \n",
    "    # Compute the gradients for each variables\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        weights3 -= 0.1*weights3.grad\n",
    "        weights2 -= 0.1*weights2.grad\n",
    "        weights1 -= 0.1*weights1.grad\n",
    "        b3 -= 0.1*b3.grad\n",
    "        b2 -= 0.1*b2.grad\n",
    "        b1 -= 0.1*b1.grad\n",
    "\n",
    "\n",
    "    it += 1\n",
    "    if it % 10 == 0:\n",
    "        test(weights1, weights2,weights3,b1, b2, b3, test_loader)\n",
    "        \n",
    "#     if it > 50000:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1], dtype=torch.int32)\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "batch_idx, (data, target) = next(enumerate(test_loader))\n",
    "data = data.float()\n",
    "# for i in data[0]:\n",
    "#     print(i)\n",
    "print(target)\n",
    "z1 = torch.matmul(data, weights1) + b1\n",
    "a1 = sigmoid_activation(z1)       \n",
    "z2 = torch.matmul(a1, weights2) + b2\n",
    "a2 = sigmoid_activation(z2)\n",
    "z3 = torch.matmul(a2, weights3) + b3      \n",
    "outputs = sigmoid_activation(z3)\n",
    "softmax = F.softmax(outputs, dim=1)\n",
    "pred = softmax.argmax(dim=1, keepdim=True)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:2.1m;\"> 1- Modèle de prediction 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:1.5m;\"> 1- Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /info/etu/m2/s185313/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 14:43:53,726 - spacy_lefff.lefff - INFO - New LefffLemmatizer instantiated.\n",
      "2019-11-06 14:43:53,727 - spacy_lefff.lefff - INFO - Reading lefff data...\n",
      "2019-11-06 14:43:54,046 - spacy_lefff.lefff - INFO - Successfully loaded lefff lemmatizer\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('french')\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "french_lemmatizer = LefffLemmatizer()\n",
    "nlp.add_pipe(french_lemmatizer, name='lefff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier = open(\"CorpusM2-AFD/corpus.tache1.learn\",\"r\",encoding = \"latin-1\")\n",
    "# fichier = open(\"CorpusM2-AFD/corpus.tache1Reduit.learn\",\"r\",encoding = \"latin-1\")\n",
    "\n",
    "lignes = fichier.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for ligne in lignes:\n",
    "    ligne = ligne.replace('\\n','')\n",
    "    data.append(ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.snowball import FrenchStemmer\n",
    "# stemmer = FrenchStemmer()\n",
    "# print(data[0])\n",
    "# print(data[0].split(' ')[3])\n",
    "# print(stemmer.stem(data[0].split(' ')[3]))\n",
    "# print(stemmer.stem('diplomatique'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroOuUn(c):\n",
    "    if c == 'C':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupPersonne(s):\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i]=='>':\n",
    "            return zeroOuUn(s[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recupDonnees():\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for doc in data:\n",
    "#         doc1 = doc.split(' ')\n",
    "# #         doc1 =  [stemmer.stem(m) for m in doc1] #lemmetisation\n",
    "#     #     print(\" \".join(doc1[1:]))\n",
    "#         res = \" \".join(doc1[1:])        \n",
    "#         res = res.lower() #convertit les Majuscules en miniscules\n",
    "# #         print(res)\n",
    "#         doc_lem = nlp(res)\n",
    "#         phrase = \"\"\n",
    "#         for d in doc_lem:\n",
    "#             phrase += d.lemma_ +\" \"\n",
    "#         res = phrase\n",
    "#         res = res.split(' ')\n",
    "#         res = \" \".join([mot for mot in res if mot not in stopwords]) #suppression des stopwords\n",
    "#         res = unidecode(res) #suppression des accents et cedilles\n",
    "#         res = res.translate(str.maketrans('', '', string.punctuation)) #suprression des ponctuations\n",
    "        \n",
    "# #         print(res)\n",
    "#         X.append(res)\n",
    "#         y.append(recupPersonne(doc1[0]))\n",
    "# #         print(recupPersonne(doc1[0]))\n",
    "# #         break\n",
    "#     return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y = recupDonnees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupDonnees2():\n",
    "    X = []\n",
    "    y = []\n",
    "    f = open(\"data.txt\",\"r\")\n",
    "    lignes = f.readlines()\n",
    "    i = 0\n",
    "    for ligne in lignes:\n",
    "        i += 1\n",
    "        ligne = ligne.replace('\\n','')\n",
    "        doc = ligne.split(',')\n",
    "        X.append(\" \".join(doc[1:]))   \n",
    "        y.append(zeroOuUn(doc[0]))\n",
    "       \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = recupDonnees2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(y[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"J'épargnerai à mes lecteurs importante la liste complète de ces âpres négociations pour ne d'en sortir.\"\n",
    "# text = text.lower() #convertit les Majuscules en miniscules\n",
    "# doc_lem = nlp(text)\n",
    "# phrase = \"\"\n",
    "# for d in doc_lem:\n",
    "#     phrase += d.lemma_ +\" \"\n",
    "\n",
    "# text = phrase\n",
    "# text = text.split(' ')\n",
    "# text = \" \".join([mot for mot in text if mot not in stopwords]) #suppression des stopwords\n",
    "# text = unidecode(text) #suppression des accents et cedilles\n",
    "# text = text.translate(str.maketrans('', '', string.punctuation)) #suprression des ponctuations\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# SEED = 987654321\n",
    "# random.seed(SEED)\n",
    "# random.shuffle(X)\n",
    "# random.shuffle(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -  vouloir  ici  saluer ministre cooperation   nom   ancien president  lorsque etre depute  groupe france  gabon  temoigne interet qu avoir toujours porter gabon  plus  etre ami long date  \n"
     ]
    }
   ],
   "source": [
    "print(y[100], \" - \",X[39904])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  -  quand dire cher ami  agir la formule diplomatique  expression ressentir  \n",
      "1  -  avoir brazzaville  afrique demain dessine  \n",
      "0  -  etre vrai avoir tres souvent pays refus regarder droit devant soi  comme souhait rester la  certain peur changement  etre vrai  \n"
     ]
    }
   ],
   "source": [
    "print(y[0], \" - \",X[0])\n",
    "print(y[10], \" - \",X[10])\n",
    "print(y[12], \" - \",X[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57413, 143581)\n"
     ]
    }
   ],
   "source": [
    "mot_max = 10000\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,5),min_df=2)\n",
    "# vectorizer = TfidfVectorizer(max_features=mot_max,min_df=5, max_df=0.7)\n",
    "# vectorizer = TfidfVectorizer(min_df=5, max_df=0.7)\n",
    "# vectorizer = TfidfVectorizer(max_features=mot_max,ngram_range=(2,2),min_df=3, max_df=0.7)\n",
    "# vectorizer = TfidfVectorizer(max_features=mot_max, min_df=5, max_df=0.7)\n",
    "X_tfifd = vectorizer.fit_transform(X)\n",
    "print(X_tfifd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.309112265783739\n",
      "3.4353471442597003\n",
      "3.9990593249904043\n",
      "2.146956716267915\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(X_tfifd[0].toarray()))\n",
    "print(np.sum(X_tfifd[1].toarray()))\n",
    "\n",
    "print(np.sum(X_tfifd[15].toarray()))\n",
    "print(np.sum(X_tfifd[16].toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorizer.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# n = 1000\n",
    "# svd = TruncatedSVD(n_components=n)\n",
    "# X_tfifd = svd.fit(X_tfifd).transform(X_tfifd)\n",
    "# X_test = svd.fit(X_test).transform(X_test)\n",
    "# # X_train_svd = X_train\n",
    "# X_test_svd = X_test\n",
    "# print(xres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfifd, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45930, 143581)\n",
      "(11483, 143581)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Resultats ##################\n",
    "#(1,1) 1000 : 1324, 1322\n",
    "#(1,1) min(3) 1000 : 1314, 1308\n",
    "#(1,2) 1000 : 1328, 1308\n",
    "#(2,3) 1000 : 1447, 1434\n",
    "#(2,3) 1000 : 1476, 1477\n",
    "#(1,5) min(2) : 1168, 1071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- %d (11483, 950)\n",
      "[[  355   808]\n",
      " [  142 10178]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.31      0.43      1163\n",
      "          1       0.93      0.99      0.96     10320\n",
      "\n",
      "avg / total       0.90      0.92      0.90     11483\n",
      "\n",
      "0.9172690063572237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "clf_LOG = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
    "clf_LOG.fit(X_train, y_train)\n",
    "y_pred_1 = clf_LOG.predict(X_test)\n",
    "# y_pred_1 = clf_LOG.predict_proba(X_test)\n",
    "\n",
    "clf_LOG.score(X_test, y_test)\n",
    "print(\"---- %d\", (X_test.shape[0],(y_test != y_pred_1).sum()))\n",
    "print(confusion_matrix(y_test,y_pred_1))\n",
    "print(classification_report(y_test,y_pred_1))\n",
    "print(accuracy_score(y_test, y_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n",
      "---------------------------------------------------------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(y_test[:333]))\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(y_pred_1[:333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- %d (11483, 953)\n",
      "[[  484   679]\n",
      " [  274 10046]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.42      0.50      1163\n",
      "          1       0.94      0.97      0.95     10320\n",
      "\n",
      "avg / total       0.91      0.92      0.91     11483\n",
      "\n",
      "0.9170077505878255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_SVC = LinearSVC()\n",
    "\n",
    "clf_SVC.fit(X_train, y_train)\n",
    "\n",
    "y_pred_2 = clf_SVC.predict(X_test)\n",
    "clf_SVC.score(X_test, y_test)\n",
    "print(\"---- %d\", (X_test.shape[0],(y_test != y_pred_2).sum()))\n",
    "print(confusion_matrix(y_test,y_pred_2))\n",
    "print(classification_report(y_test,y_pred_2))\n",
    "print(accuracy_score(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n",
      "---------------------------------------------------------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(y_test[:333]))\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(y_pred_2[:333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- %d (11483, 1178)\n",
      "[[  136  1027]\n",
      " [  151 10169]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.12      0.19      1163\n",
      "          1       0.91      0.99      0.95     10320\n",
      "\n",
      "avg / total       0.86      0.90      0.87     11483\n",
      "\n",
      "0.8974135678829575\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# # classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "# classifier = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred_3 = classifier.predict(X_test)\n",
    "# classifier.score(X_test, y_test)\n",
    "# print(\"---- %d\", (X_test.shape[0],(y_test != y_pred_3).sum()))\n",
    "# print(confusion_matrix(y_test,y_pred_3))\n",
    "# print(classification_report(y_test,y_pred_3))\n",
    "# print(accuracy_score(y_test, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n",
      "---------------------------------------------------------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# print(np.asarray(y_test[:333]))\n",
    "# print(\"---------------------------------------------------------------------------\")\n",
    "# print(y_pred_3[:333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.utils.data as utils\n",
    "# from torchvision import datasets, transforms\n",
    "# import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_tensor_type(torch.FloatTensor)\n",
    "# # print(torch.get_default_dtype())\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# # train_data_tensor = torch.tensor(X_train_tfifd.toarray())\n",
    "# train_data_tensor = torch.tensor(X_train)\n",
    "# train_target_tensor = torch.tensor(np.asarray(y_train)) # transform to torch tensors\n",
    "\n",
    "# # test_data_tensor = torch.tensor(X_test_tfifd.toarray())\n",
    "# test_data_tensor = torch.tensor(X_test)\n",
    "# test_target_tensor = torch.tensor(np.asarray(y_test)) # transform to torch tensors\n",
    "\n",
    "# train_dataset = utils.TensorDataset(train_data_tensor,train_target_tensor) # create your datset\n",
    "# test_dataset = utils.TensorDataset(test_data_tensor,test_target_tensor) # create your datset\n",
    "\n",
    "# batch_size = 50\n",
    "# # Set the training loader\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# # Set the testing loader\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights1 = torch.randn(n, 5, requires_grad=True)\n",
    "# weights2 = torch.randn(5, 2, requires_grad=True)\n",
    "\n",
    "# b1 = torch.randn((1, 5), requires_grad=True) # bias for hidden layer\n",
    "# b2 = torch.randn((1, 2), requires_grad=True) # bias for output layer\n",
    "\n",
    "# # b1 = torch.randn(500,)\n",
    "# # b2 = torch.randn((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid_activation(z):\n",
    "#     return 1 / (1 + torch.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(weights1, weights2,b1, b2, test_loader):\n",
    "#     test_size = len(test_loader.dataset)\n",
    "#     correct = 0\n",
    "\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         #print(batch_idx, data.shape, target.shape)\n",
    "# #         data = data.view((-1, 28*28))\n",
    "#         #print(batch_idx, data.shape, target.shape)\n",
    "#         data = data.float()\n",
    "#         target = target.to(dtype=torch.long)        \n",
    "#         z1 = torch.matmul(data, weights1) + b1\n",
    "#         a1 = sigmoid_activation(z1)\n",
    "        \n",
    "#         z2 = torch.matmul(a1, weights2) + b2\n",
    "        \n",
    "#         outputs = sigmoid_activation(z2)\n",
    "        \n",
    "#         softmax = F.softmax(outputs, dim=1)\n",
    "#         pred = softmax.argmax(dim=1, keepdim=True)\n",
    "#         n_correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "#         correct += n_correct\n",
    "\n",
    "#     acc = correct / test_size\n",
    "#     print(\" Accuracy on test set\", acc)\n",
    "#     return\n",
    "\n",
    "# test(weights1, weights2,b1, b2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = 0\n",
    "# for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "#     # Be sure to start the loop with zeros grad\n",
    "#     if weights1.grad is not None:\n",
    "#         weights1.grad.zero_()\n",
    "    \n",
    "#     if weights2.grad is not None:\n",
    "#         weights2.grad.zero_()\n",
    "        \n",
    "    \n",
    "#     if b1.grad is not None:\n",
    "#         b1.grad.zero_()\n",
    "        \n",
    "#     if b2.grad is not None:\n",
    "#         b2.grad.zero_()\n",
    "\n",
    "        \n",
    "        \n",
    "#     #print(\"batch_idx: {}, data.shape: {}, target.shape: {}\".format(batch_idx, data.shape, targets.shape))\n",
    "#     data = data.float()\n",
    "#     targets = targets.to(dtype=torch.long)\n",
    "#     z1 = torch.matmul(data, weights1) + b1\n",
    "#     a1 = sigmoid_activation(z1)\n",
    "        \n",
    "#     z2 = torch.matmul(a1, weights2) + b2\n",
    "    \n",
    "#     outputs = sigmoid_activation(z2)\n",
    "#     log_softmax = F.log_softmax(outputs, dim=1)\n",
    "#     #print(\"Log softmax: {}\".format(log_softmax.shape))\n",
    "\n",
    "#     #print((-log_softmax[0][targets[0]] + -log_softmax[1][targets[1]] )  / 2 )\n",
    "#     #print(-log_softmax[0][targets[0]], targets[0])\n",
    "    \n",
    "#     loss = F.nll_loss(log_softmax, targets)\n",
    "#     print(\"\\rLoss shape: {}\".format(loss), end=\"\")\n",
    "    \n",
    "#     # Compute the gradients for each variables\n",
    "#     loss.backward()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         weights2 -= 0.1*weights2.grad\n",
    "#         weights1 -= 0.1*weights1.grad\n",
    "#         b2 -= 0.1*b2.grad\n",
    "#         b1 -= 0.1*b1.grad\n",
    "\n",
    "\n",
    "#     it += 1\n",
    "#     if it % 10 == 0:\n",
    "#         test(weights1, weights2,b1, b2, test_loader)\n",
    "        \n",
    "# #     if it > 50000:\n",
    "# #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_idx, (data, target) = next(enumerate(test_loader))\n",
    "# data = data.float()\n",
    "# # for i in data[0]:\n",
    "# #     print(i)\n",
    "# print(target)\n",
    "# z1 = torch.matmul(data, weights1) + b1\n",
    "# a1 = sigmoid_activation(z1)       \n",
    "# z2 = torch.matmul(a1, weights2) + b2\n",
    "# outputs = sigmoid_activation(z2)\n",
    "# softmax = F.softmax(outputs, dim=1)\n",
    "# pred = softmax.argmax(dim=1, keepdim=True)\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
